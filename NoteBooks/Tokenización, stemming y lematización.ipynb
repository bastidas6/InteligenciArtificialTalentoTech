{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b47054",
   "metadata": {},
   "source": [
    "# Tokenizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1583beb",
   "metadata": {},
   "source": [
    "Para construir un vocabulario, lo primero que hay que hacer es dividir los documentos o las frases en fragmentos llamados tokens . Cada token lleva asociado un significado semántico. La tokenización es una de las cosas fundamentales que se deben hacer en cualquier actividad de procesamiento de texto. La tokenización puede considerarse como una técnica de segmentación en la que se intenta dividir fragmentos de texto más grandes en fragmentos más pequeños y significativos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41585c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La', 'capital', 'de', 'Colombia', 'es', 'Bogotá']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"La capital de Colombia es Bogotá\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54497d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a873f95",
   "metadata": {},
   "source": [
    "## Tokenizador de expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e109cdd",
   "metadata": {},
   "source": [
    "El paquete nltk de Python proporciona tokenizadores basados en expresiones regulares. Por ejemplo RegexpTokenizer Se puede utilizar para tokenizar o dividir una oración en función de una expresión regular proporcionada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3275d3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Un',\n",
       " 'computador',\n",
       " 'cuesta',\n",
       " 'entr',\n",
       " '$2.000.000',\n",
       " '-',\n",
       " '$10.000.000',\n",
       " 'En',\n",
       " 'Colombia',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"Un computador cuesta entr $2.000.000 - $10.000.000 En Colombia.\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b1cba",
   "metadata": {},
   "source": [
    "## Tokenizador TreeBank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a70b10",
   "metadata": {},
   "source": [
    "El tokenizador de Treebank también utiliza expresiones regulares para tokenizar texto según Penn Treebank ( https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html ). Aquí, las palabras se dividen principalmente en función de la puntuación.\n",
    "\n",
    "El tokenizador de Treebank hace un gran trabajo al separar contracciones. Además, identifica los puntos al final de las líneas y los elimina. La puntuación, como las comas, se separa si va seguida de espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b1cb19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'that',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'cost',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '3.000.000']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from nltk.tokenize import TreebankWordTokenizer\n",
    " s = \"I'm going to buy a computer that doesn't cost more than $3.000.000\"\n",
    " tokenizer = TreebankWordTokenizer()\n",
    " tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80887a86",
   "metadata": {},
   "source": [
    "## Tokenizador de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5387147",
   "metadata": {},
   "source": [
    "Usado para analizar emoticones, hashtags y texto abreviado de por ejemplo redes sociales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "569220d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@cmpineda',\n",
       " \"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Computer',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#pcs']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@cmpineda I'm going to buy a Computer!!! :-D #happiness #pcs\"\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092f6f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Computer',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#pcs']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intenta reducir los caracteres excesivos en un token\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@cmpineda I'm going to buy a Computer!!! :-D #happiness #pcs\"\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025a7dd",
   "metadata": {},
   "source": [
    "## Stemming y Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece583fe",
   "metadata": {},
   "source": [
    "Hace un intento rudimentario de eliminar las formas flexivas de una palabra y llevarlas a una forma base llamada stem.\n",
    "\n",
    "Los dos algoritmos o métodos más comunes que se emplean para la lematización son el lematizador Porter y el lematizador Snowball . El lematizador Porter admite el idioma inglés, mientras que el lematizador Snowball, que es una mejora del lematizador Porter, admite varios idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86562d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ed85627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
     ]
    }
   ],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
    " 'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously'] \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b0c55bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call arbol niñ mujer person\n"
     ]
    }
   ],
   "source": [
    "plurales = ['calles', 'arboles', 'niños', 'mujeres', 'personas'] \n",
    "\n",
    "stemmer2 = SnowballStemmer(language='spanish')\n",
    "singles = [stemmer2.stem(plural) for plural in plurales]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f98df",
   "metadata": {},
   "source": [
    "### Lematizador de WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8e19e",
   "metadata": {},
   "source": [
    "WordNet es una base de datos léxica del inglés que está disponible de forma gratuita y pública. Como parte de WordNet, los sustantivos, verbos, adjetivos y adverbios se agrupan en conjuntos de sinónimos cognitivos (synsets), cada uno de los cuales expresa conceptos distintos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73357b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los tokens son:  ['We', 'are', 'studying', 'NLP', 'techniques']\n",
      "La salida de la lematización es:  We are studying NLP technique\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Carlos\n",
      "[nltk_data]     Pineda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lematizador = WordNetLemmatizer()\n",
    "s = \"We are studying NLP techniques\"\n",
    "lista_tokens = s.split()\n",
    "print(\"Los tokens son: \", lista_tokens)\n",
    "salida_lematizada = ' '.join([lematizador.lemmatize(token) for token in lista_tokens])\n",
    "print(\"La salida de la lematización es: \", salida_lematizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74aa6cc",
   "metadata": {},
   "source": [
    "El lematizador de WordNet funciona mejor si las etiquetas POS también se proporcionan como entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62d89dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Carlos Pineda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('studying', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('techniques', 'NNS')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tags = nltk.pos_tag(lista_tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a82c74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_pos_tag(token):\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7b70e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We be study NLP technique\n"
     ]
    }
   ],
   "source": [
    "salida = [lemmatizer.lemmatize(token, get_pos_tag(token)) for token in lista_tokens]\n",
    "print(' '.join(salida))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff072424",
   "metadata": {},
   "source": [
    "## Eliminando palabras vacías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d531f",
   "metadata": {},
   "source": [
    "Las palabras vacías son palabras como un, la , en, etc, que aparecen con frecuencia en los corpus de texto y no aportan mucha información en la mayoría de los contextos. Estas palabras, en general, son necesarias para completar oraciones y hacer que sean gramaticalmente correctas. Suelen ser las palabras más comunes de un idioma y se pueden filtrar en la mayoría de las tareas de PNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "530bc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Carlos\n",
      "[nltk_data]     Pineda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"we, wasn, t, itself, himself, such, below, few, once, haven, isn't, being, to, then, hasn't, is, are, more, been, where, down, it, weren't, of, because, theirs, that, mustn, as, my, mustn't, their, am, ourselves, after, no, isn, didn't, weren, having, s, now, ain, he, themselves, ve, each, y, yours, from, wouldn't, which, again, haven't, them, be, ma, with, you'll, in, she, doesn't, at, why, all, his, about, aren, during, only, herself, before, o, ll, him, myself, have, mightn't, that'll, than, needn't, should've, was, does, whom, against, will, nor, most, has, wasn't, any, yourselves, couldn, its, or, hers, when, until, doing, this, didn, above, off, very, these, other, our, by, needn, own, through, it's, further, don, for, couldn't, did, hadn, can, ours, aren't, not, you're, your, hasn, should, won't, shan, into, don't, some, a, up, had, shouldn, me, shan't, you'd, won, between, same, but, d, re, here, doesn, she's, just, hadn't, both, her, and, an, who, too, i, yourself, while, wouldn, m, there, you've, how, were, mightn, shouldn't, you, they, what, out, so, those, if, on, over, under, do, the\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\", \".join(stop) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b167b1",
   "metadata": {},
   "source": [
    "Sin embargo, estas palabras son muy significativas en casos de uso como la respuesta a preguntas y la clasificación de preguntas. Se deben tomar medidas para garantizar que estas palabras no se filtren cuando el corpus de texto se someta a la eliminación de palabras vacías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c9cf95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how putting efforts enhance understanding NLP'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_pregunta = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "stop = set(stopwords.words('english'))\n",
    "oracion = \"how are we putting in efforts to enhance our understanding of NLP\"\n",
    "for palabra in palabras_pregunta:\n",
    "    stop.remove(palabra)\n",
    "salida = [token for token in oracion.split() if token not in stop]\n",
    "\" \".join(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffec49",
   "metadata": {},
   "source": [
    "## N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a75047",
   "metadata": {},
   "source": [
    "Las oraciones generalmente contienen nombres de personas y lugares y otros términos compuestos como:\n",
    "sala de estar y taza de café. Se agrupan bajo el término general de n -gramas. Cuando n es igual a 1, se denominan unigramas. Los bigramas , o 2-gramas, se refieren a pares de palabras, como mesa del comedor. Las frases como Emiratos Árabes Unidos que comprenden tres palabras se denominan trigramas o 3-gramas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45365a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language',\n",
       " 'Language Processing',\n",
       " 'Processing a',\n",
       " 'a very',\n",
       " 'very interesting']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "s = \"Natural Language Processing a very interesting\"\n",
    "tokens = s.split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "[\" \".join(token) for token in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded95539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
